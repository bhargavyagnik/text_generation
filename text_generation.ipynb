{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQ2+IlKF/AjqUFLfW7zNvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavyagnik/text_generation/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9QrR5phhTSt"
      },
      "source": [
        "# Text Generation using Transformers library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAG7OH5qgqWH"
      },
      "source": [
        "This work is based on the Huggingface tutorial titled \"How to generate text: using different decoding methods for language generation with Transformers\" by  [Patrick Von Platen](https://huggingface.co/patrickvonplaten). \r\n",
        "\r\n",
        "I highly recommend you to check that ! It is quite detailed and very well explained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDGOp5fOhSgX",
        "outputId": "027c9d57-b1e4-4255-c632-fbbebf3fd632"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\r\n",
        "!pip install -q tensorflow==2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 890kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 17.8MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 37kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 49.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjaIPGqFhQ-t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}